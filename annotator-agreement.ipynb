{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring Annotator Agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a companion to a [blog post](https://pwsiegel.github.io/ds/iaa/) that I wrote about the problem of measuring inter-annotator agreement in machine learning experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In short, it is standard practice to use human annotators to produce gold standard training or validation data for machine learning algorithms, but one runs into trouble in annotation experiments where humans disagree frequently.\n",
    "High levels of disagreement often indicates that the annotation task is vague or subjective, and this potentially means that the function being measured is not sufficiently well-defined for a statistical model to be successful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when conducting annotation experiments it is important to measure the rate of agreement in the results.\n",
    "But designing an adequate metric which can be compared across experiments with different numbers of annotators assigning different numbers of categories to data is not trivial, and the standard approach (the so-called $\\kappa$ statistics) are unsatisfactory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my post I argued that it is better to compute agreement scores for each category in the experiment separately and then aggregate those scores if necessary.\n",
    "I proposed an algorithm for carrying out this computation, and the purpose of this repository is to implement that algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Annotator Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, let us simulate some annotation experiments.\n",
    "The first will have high agreement across the board, the second will have low agreement across the board, and the third will have low agreement in one category but high agreement in the others.\n",
    "In each experiment three annotators $A$, $B$, and $C$ will apply category labels $0$, $1$, and $2$ to 1000 data points.\n",
    "\n",
    "- In the first experiment all three annotators will apply the label $i \\mod 3$ to the data point indexed $i$ with probability $.95$.\n",
    "- In the second experiment annotators $A$ and $B$ will apply the label $i \\mod 3$ and annotator $C$ will apply the label $i+1 \\mod 3$ to the data point indexed $i$ with probability $.95$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(index, value):\n",
    "    if value <= .95:\n",
    "        return index % 3\n",
    "    elif value <= .975:\n",
    "        return (index + 1) % 3\n",
    "    else:\n",
    "        return (index + 2) % 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "high = [\n",
    "    [model(i, random.random()), model(i, random.random()), model(i, random.random())]\n",
    "    for i in range(1000)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "low = [\n",
    "    [model(i, random.random()), model(i, random.random()), model(i+1, random.random())]\n",
    "    for i in range(1000)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mixed = []\n",
    "\n",
    "for i in range(1000):\n",
    "    A = model(i, random.random())\n",
    "    B = model(i, random.random())\n",
    "    C = model(i, random.random()) if A == 0 else 1\n",
    "    mixed.append([A, B, C])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agreement by Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def agreement_rate_by_category(data):\n",
    "    agreement = defaultdict(int)\n",
    "    potential = defaultdict(int)\n",
    "    \n",
    "    for row in data:\n",
    "        for cat in set(row):\n",
    "            count = row.count(cat)\n",
    "            agreement[cat] += choose2(count)\n",
    "            potential[cat] += count * len(row) - choose2(count + 1)\n",
    "    \n",
    "    return {cat: agreement[cat] / potential[cat] for cat in agreement}\n",
    "\n",
    "def choose2(n):\n",
    "    return n*(n-1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.8295142071494043, 1: 0.8324225865209471, 2: 0.8359447004608295}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agreement_rate_by_category(high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.19442761962447003, 1: 0.19830713422007254, 2: 0.2042377869334903}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agreement_rate_by_category(low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.8260869565217391, 1: 0.555045871559633, 2: 0.2853080568720379}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agreement_rate_by_category(mixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
